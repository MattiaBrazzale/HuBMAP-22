{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattiaBrazzale/HuBMAP-22/blob/main/model/CoaT_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuBMAP-22 Challenge\n",
        "\n",
        "This is my notebook with the training and submitting for the HuBMAP-22 Challenge.\n",
        "\n",
        "The training is done by performing a 5-fold split, and using 4/5 folds for training and 1/5 fold for validation.\n"
      ],
      "metadata": {
        "id": "6v22Y8t6pqbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements\n",
        "Installing the required packages:"
      ],
      "metadata": {
        "id": "zbKUqAG7pyRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -qq git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "!pip install timm==0.4.12\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "b0KwsgMapx7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading libraries and utilities:"
      ],
      "metadata": {
        "id": "ZSWc9B269Ow-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "from sklearn.model_selection import KFold\n",
        "import segmentation_models_pytorch as smp\n",
        "import tifffile\n",
        "import torchvision\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from coat import *\n",
        "from daformer import *\n",
        "from utils import seed_everything, make_fold, get_mask, rle_encode\n",
        "import config\n",
        "seed_everything(config.SEED)"
      ],
      "metadata": {
        "id": "nHcMI6CTxO5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataframes and splitting the train set into training and validation set:"
      ],
      "metadata": {
        "id": "_SwQg3LPAFfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(config.TRAIN_PATH+'train.csv')\n",
        "test_df = pd.read_csv(config.TEST_PATH+'test.csv')\n",
        "train_df, val_df = make_fold(num_fold=config.NUM_FOLD, val_fold=config.VAL_FOLD, df=train)"
      ],
      "metadata": {
        "id": "MQYvh1bpAFJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the PyTorch dataset for the images:"
      ],
      "metadata": {
        "id": "4SYY6HyTm1qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuBMAPData(Dataset):\n",
        "    def __init__(self, transform=None, df=train_df):\n",
        "        self.transform = transform\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        temp = self.df.iloc[index]\n",
        "        id = temp['id']\n",
        "        organ = temp['organ']\n",
        "\n",
        "        image = tifffile.imread(config.TRAIN_PATH+str(id)+'.tiff')\n",
        "        mask = get_mask(id, self.df)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "            mask = mask.float()\n",
        "\n",
        "        return image, mask, organ"
      ],
      "metadata": {
        "id": "OW5T0sLOBWDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definining the augmentations for the training set, that we need since we have only a small amount of images:"
      ],
      "metadata": {
        "id": "ET13KBvgm-qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = A.Compose(\n",
        "      [\n",
        "        A.Resize(height=config.IMG_SIZE, width=config.IMG_SIZE),\n",
        "        A.Rotate(limit=35, p=0.8),\n",
        "        A.OneOf([\n",
        "            A.HorizontalFlip(p=0.6),\n",
        "            A.VerticalFlip(p=0.6),\n",
        "            A.RandomRotate90(p=0.6)\n",
        "        ], p=1.0),\n",
        "        A.OneOf([\n",
        "            A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
        "            A.GridDistortion(p=0.5),\n",
        "            A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=0.5) \n",
        "        ], p=0.8),\n",
        "        A.ChannelShuffle(p=0.4),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.4),\n",
        "        A.OneOf([\n",
        "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=0.75),\n",
        "            A.RandomBrightnessContrast(p=0.7),\n",
        "            A.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.25, p=0.75)\n",
        "        ], p=0.9),\n",
        "        A.RandomGamma(p=0.6),\n",
        "        ToTensorV2(transpose_mask=True),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=config.IMG_SIZE, width=config.IMG_SIZE),\n",
        "        ToTensorV2(transpose_mask=True),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "elZ1Bdd_Bizv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the datasets and the dataloaders:"
      ],
      "metadata": {
        "id": "lwzgrCVwnO9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = HuBMAPData(transform=train_transform, df=train_df)\n",
        "val_dataset = HuBMAPData(transform=val_transform, df=val_df)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=config.BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=config.BATCH_SIZE)"
      ],
      "metadata": {
        "id": "sru3YutIBnNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoaT \n",
        "\n",
        "Loading the pretrained weights:\n",
        "\n"
      ],
      "metadata": {
        "id": "zTQHrR0T_F5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./coat-pretrained\n",
        "!wget http://vcl.ucsd.edu/coat/pretrained/coat_lite_medium_a750cd63.pth -P ./coat-pretrained\n",
        "!sha256sum ./coat-pretrained/coat_lite_medium_a750cd63.pth"
      ],
      "metadata": {
        "id": "gomVx6RfnyBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model architecture and the function to initialize the model:"
      ],
      "metadata": {
        "id": "xhfOHu7Znun7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RGB(nn.Module):\n",
        "    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406]  \n",
        "    IMAGE_RGB_STD = [0.229, 0.224, 0.225] \n",
        "    \n",
        "    def __init__(self, ):\n",
        "        super(RGB, self).__init__()\n",
        "        self.register_buffer('mean', torch.zeros(1, 3, 1, 1))\n",
        "        self.register_buffer('std', torch.ones(1, 3, 1, 1))\n",
        "        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n",
        "        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x\n",
        "\n",
        "    \n",
        "    \n",
        "class Net(nn.Module):\n",
        "\t\n",
        "\t\tdef __init__(self,\n",
        "\t\t\t\t\t\t\t\tencoder=coat_lite_medium,\n",
        "\t\t\t\t\t\t\t\tdecoder=daformer_conv3x3,\n",
        "\t\t\t\t\t\t\t\tencoder_cfg={},\n",
        "\t\t\t\t\t\t\t\tdecoder_cfg={},\n",
        "\t\t\t\t\t\t\t\t):\n",
        "\t\t\t\tsuper(Net, self).__init__()\n",
        "\t\t\t\tdecoder_dim = decoder_cfg.get('decoder_dim', 320)\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.rgb = RGB()\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.encoder = encoder\n",
        "\t\t\t\tencoder_dim = self.encoder.embed_dims\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.decoder = decoder(\n",
        "\t\t\t\t\t\tencoder_dim=encoder_dim,\n",
        "\t\t\t\t\t\tdecoder_dim=decoder_dim,\n",
        "\t\t\t\t)\n",
        "\t\t\t\tself.logit = nn.Sequential(\n",
        "\t\t\t\t\t\tnn.Conv2d(decoder_dim, 1, kernel_size=1),\n",
        "\t\t\t\t\t\tnn.Upsample(scale_factor = 4, mode='bilinear', align_corners=False),\n",
        "\t\t\t\t)\n",
        "\t\t\n",
        "\t\tdef forward(self, batch):\n",
        "\t\t\t\n",
        "\t\t\t\tx = self.rgb(batch)\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\tB, C, H, W = x.shape\n",
        "\t\t\t\tencoder = self.encoder(x)\n",
        "\t\t\t\t\n",
        "\t\t\t\tlast, decoder = self.decoder(encoder)\n",
        "\t\t\t\tlogit = self.logit(last)\n",
        "\t\t\t\t\n",
        "\t\t\t\toutput = {}\n",
        "\t\t\t\tprobability_from_logit = torch.sigmoid(logit)\n",
        "\t\t\t\toutput['probability'] = probability_from_logit\n",
        "\t\t\t\t\n",
        "\t\t\t\treturn output\n",
        "\n",
        "\n",
        "def init_model():\n",
        "\t\t\"\"\"\n",
        "\t\tFunction used to initialize a CoaT model\n",
        "\t\t\"\"\"\n",
        "    encoder = coat_lite_medium()\n",
        "    checkpoint = './coat-pretrained/coat_lite_medium_a750cd63.pth' #pretrained weight available at the CoaT repository\n",
        "    checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "    state_dict = checkpoint['model']\n",
        "    encoder.load_state_dict(state_dict,strict=False)\n",
        "    net = Net(encoder=encoder).cuda()\n",
        "    return net"
      ],
      "metadata": {
        "id": "9MGHTZHSsJW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definining the class for the custom loss and the evaluation metric:"
      ],
      "metadata": {
        "id": "qLdjD9HIAkFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss,self).__init__()\n",
        "        self.diceloss = smp.losses.DiceLoss(mode='binary')\n",
        "        self.binloss = smp.losses.SoftBCEWithLogitsLoss(reduction = 'mean' , smooth_factor = 0.1)\n",
        "        self.jaccardloss = smp.losses.JaccardLoss(mode='binary')\n",
        "\n",
        "    def forward(self, output, mask):\n",
        "        dice = self.diceloss(outputs, mask)\n",
        "        bce = self.binloss(outputs, mask)\n",
        "        jaccard = self.jaccardloss(outputs, mask)\n",
        "        loss = dice * 0.3 + jaccard * 0.7\n",
        "        return loss\n",
        "\n",
        "class DiceCoef(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, smooth=1.):\n",
        "        y_true = y_true.view(-1)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        \n",
        "        y_pred = torch.round((y_pred - y_pred.min()) / (y_pred.max() - y_pred.min()))\n",
        "        \n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        dice = (2.0*intersection + smooth)/(y_true.sum() + y_pred.sum() + smooth)\n",
        "        \n",
        "        return dice"
      ],
      "metadata": {
        "id": "dpzsTBBxsJRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "FpTAgipFBevm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the TRAIN variable to True in the config file to perform training\n",
        "if config.TRAIN:\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_scores=[]\n",
        "    val_scores = []\n",
        "    best_loss = 999\n",
        "    best_score = 0\n",
        "\n",
        "    model = init_model().to(config.DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam([\n",
        "        {'params': model.decoder.parameters(), 'lr': config.DECODER_LEARNING_RATE}, \n",
        "        {'params': model.encoder.parameters(), 'lr': config.ENCODER_LEARNING_RATE},  \n",
        "    ])\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, \n",
        "                                              max_lr=MAX_LEARNING_RATE,\n",
        "                                              epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
        "\n",
        "    loss_func = CustomLoss()\n",
        "    dice_coe = DiceCoef()\n",
        "\n",
        "    for epoch in tqdm(range(config.EPOCHS)):\n",
        "\n",
        "        # ------- Train ------- #\n",
        "\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        score = 0\n",
        "\n",
        "        for batch_idx, (img, mask, organ) in enumerate(train_loader):\n",
        "            \n",
        "            img = img.float().to(device=config.DEVICE)\n",
        "            mask = mask.float().to(device=config.DEVICE)\n",
        "            outputs = model(img)['probability']   \n",
        "\n",
        "            loss = loss_func(outputs, mask)\n",
        "            loss.backward()\n",
        "            \n",
        "            if ((batch_idx+1)*config.BATCH_SIZE % config.ACCUMULATION == 0) | ((batch_idx+1) == len(train_df)):\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "            train_loss += loss.detach()\n",
        "            score += dice_coe(outputs,mask).item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        score /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        train_scores.append(score)\n",
        "        print(f\"FOLD: {val_fold}, EPOCH: {epoch+1}, Train_Loss: {train_loss} , Dice Value: {score}\") #\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # ------ Validation ------ #\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            val_loss = 0\n",
        "            val_score = 0\n",
        "\n",
        "            for i, (img, mask, organ) in enumerate(val_loader):\n",
        "\n",
        "                img = img.float().to(device=config.DEVICE)\n",
        "                mask = mask.float().to(device=config.DEVICE)\n",
        "                outputs = model(img)['probability']\n",
        "\n",
        "                loss = loss_func(outputs, mask)\n",
        "                val_loss += loss.item()\n",
        "                val_score += dice_coe(outputs,mask).item()\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            val_score /= len(val_loader)\n",
        "            val_scores.append(val_score)\n",
        "\n",
        "            print(f\"FOLD: {val_fold}, EPOCH: {epoch+1}, Val_Loss: {val_loss} , Valid Dice Value: {val_score}\") \n",
        "\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if val_score > best_score:\n",
        "            best_score = val_score\n",
        "            torch.save(model.state_dict(), f\"./FOLD{fold}_best_score_epoch{epoch+1}.pth\")\n",
        "            print(f\"Saved model for best score : FOLD{fold}_best_score_epoch{epoch+1}.pth\")\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            torch.save(model.state_dict(), f\"./FOLD{fold}_best_loss_epoch{epoch+1}.pth\")\n",
        "            print(f\"Saved model for best loss : FOLD{fold}_best_loss_epoch{epoch+1}.pth\")    \n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            torch.save(model.state_dict(), f\"/kaggle/working/FOLD{fold}_epoch_{epoch+1}.pth\")\n",
        "            print(f\"Saved model for current epoch: FOLD{fold}_epoch_{epoch+1}.pth\")\n"
      ],
      "metadata": {
        "id": "TugDBkdaJVZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference on new data\n",
        "\n",
        "The following augmentation are used to perform Test Time Agumentations, i.e. performing inference on slightly different images and then averaging the predictions:"
      ],
      "metadata": {
        "id": "oKmhWkAltmKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "horizontal_flip = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE, width=config.IMG_SIZE),\n",
        "    A.HorizontalFlip(p = 1.),\n",
        "    ToTensorV2(transpose_mask=True)])\n",
        "\n",
        "vertical_flip = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE, width=config.IMG_SIZE),\n",
        "    A.VerticalFlip(p = 1.),\n",
        "    ToTensorV2(transpose_mask=True)])\n",
        "\n",
        "rotate_cw = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE, width=config.IMG_SIZE),\n",
        "    A.Rotate(limit = (-90, -90), p = 1.),\n",
        "    ToTensorV2(transpose_mask=True)])\n",
        "\n",
        "rotate_acw = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE, width=config.IMG_SIZE),\n",
        "    A.Rotate(limit = (90, 90), p = 1.),\n",
        "    ToTensorV2(transpose_mask=True)])\n",
        "\n",
        "identity_trfm = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE, width=config.IMG_SIZE),\n",
        "    A.HorizontalFlip(p = 0.),\n",
        "    ToTensorV2(transpose_mask=True)]) # does nothing\n",
        "\n",
        "pixel_level_trfms1 = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE, width=config.IMG_SIZE),\n",
        "    A.HueSaturationValue(10,15,10),\n",
        "    ToTensorV2(transpose_mask=True)])\n",
        "\n",
        "pixel_level_trfms2 = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE, width=config.IMG_SIZE),\n",
        "    A.CLAHE(clip_limit=2),\n",
        "    ToTensorV2(transpose_mask=True)])\n",
        "\n",
        "increase_size1 = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE+32, width=config.IMG_SIZE+32),\n",
        "    ToTensorV2(transpose_mask=True),])\n",
        "\n",
        "reduce_size1 = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE-32, width=config.IMG_SIZE-32),\n",
        "    ToTensorV2(transpose_mask=True),])\n",
        "\n",
        "increase_size2 = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE+64, width=config.IMG_SIZE+64),\n",
        "    ToTensorV2(transpose_mask=True),])\n",
        "\n",
        "reduce_size2 = A.Compose([\n",
        "    A.Resize(height=config.IMG_SIZE-64, width=config.IMG_SIZE-64),\n",
        "    ToTensorV2(transpose_mask=True),])\n",
        "\n",
        "\n",
        "# List of augmentations for TTA\n",
        "tta_augs = [identity_trfm,\n",
        "            horizontal_flip,\n",
        "            vertical_flip,\n",
        "            pixel_level_trfms1,\n",
        "            pixel_level_trfms2,\n",
        "            increase_size1,\n",
        "            reduce_size1,\n",
        "            increase_size2,\n",
        "            reduce_size2]\n",
        "\n",
        "# List of deaugmentations corresponding to the above augmentation list\n",
        "tta_deaugs = [None,\n",
        "              horizontal_flip,\n",
        "              vertical_flip,\n",
        "              None,\n",
        "              None,\n",
        "              None,\n",
        "              None,\n",
        "              None,\n",
        "              None]"
      ],
      "metadata": {
        "id": "dGNHz0qDtipb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organ thresholds under which the model performance is optimized:"
      ],
      "metadata": {
        "id": "ZjZCcIiRu9SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "organ_threshold = {\n",
        "    'Hubmap': {\n",
        "        'kidney'        : 0.40,\n",
        "        'prostate'      : 0.40,\n",
        "        'largeintestine': 0.40,\n",
        "        'spleen'        : 0.40,\n",
        "        'lung'          : 0.10,\n",
        "    },\n",
        "    'HPA': {\n",
        "        'kidney'        : 0.50,\n",
        "        'prostate'      : 0.50,\n",
        "        'largeintestine': 0.50,\n",
        "        'spleen'        : 0.50,\n",
        "        'lung'          : 0.10,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "FiyO0tQiu81r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the SUBMIT variable to True in the config file to perform inference on the test images\n",
        "if config.SUBMIT:\n",
        "\n",
        "    #loading the model\n",
        "    model = init_model().to(config.DEVICE)\n",
        "    model.output_type = [\"inference\"]\n",
        "    model.load_state_dict(torch.load(config.WEIGHTS_PATH),strict=False)\n",
        "    model.float()\n",
        "    model.eval()\n",
        "\n",
        "    \n",
        "    ids = []\n",
        "    rles = []\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "\n",
        "        image_id = row['id']\n",
        "        organ = row['organ']\n",
        "        data_source = row['data_source']\n",
        "        image = tifffile.imread(config.TEST_IMG+str(image_id)+'.tiff')\n",
        "        image_shape = image.shape[:2]\n",
        "        \n",
        "        if TTA:\n",
        "            tta_pred = None\n",
        "            for i, tta_aug in enumerate(tta_augs):\n",
        "                \n",
        "                augmentations = tta_aug(image=image)\n",
        "                aug_img = augmentations[\"image\"]\n",
        "                \n",
        "                x_tensor = aug_img.to(config.DEVICE).unsqueeze(0)\n",
        "                pr_mask = model(x_tensor)['probability']\n",
        "            \n",
        "                if tta_deaugs[i] is not None:\n",
        "                    pr_mask = (pr_mask.squeeze().cpu().detach().numpy())\n",
        "                    pr_mask = tta_deaugs[i](image = image, mask = pr_mask)['mask']\n",
        "                    pr_mask = pr_mask.unsqueeze(0)\n",
        "                    pr_mask = pr_mask.unsqueeze(0)\n",
        "                \n",
        "                resize_image = torchvision.transforms.Resize(image_shape)\n",
        "                resized_pr_mask = resize_image(pr_mask)  \n",
        "                resized_pr_mask = (resized_pr_mask.squeeze().cpu().detach().numpy())\n",
        "\n",
        "                if tta_pred is None:\n",
        "                    tta_pred = resized_pr_mask\n",
        "                else:       \n",
        "                    tta_pred += resized_pr_mask\n",
        "                    \n",
        "            tta_pred = tta_pred / len(tta_augs) \n",
        "            threshold = organ_threshold[data_source][organ]\n",
        "            th_mask = (tta_pred > threshold).astype(int)\n",
        "\n",
        "        else:\n",
        "            augmentations = val_transform(image=image)\n",
        "            image = augmentations[\"image\"]\n",
        "            \n",
        "            x_tensor = image.to(config.DEVICE).unsqueeze(0).float()\n",
        "            pr_mask = model(x_tensor)['probability']\n",
        "            resize_image = torchvision.transforms.Resize(image_shape)\n",
        "            resized_pr_mask = resize_image(pr_mask)\n",
        "            pr_mask = (resized_pr_mask.squeeze().cpu().detach().numpy())\n",
        "            \n",
        "            threshold = organ_threshold[data_source][organ]\n",
        "            th_mask = (pr_mask > threshold).astype(int)\n",
        "\n",
        "        rle = rle_encode(th_mask)\n",
        "        ids.append(image_id)\n",
        "        rles.append(rle)\n",
        "\n",
        "    submission_df = pd.DataFrame({'id':ids,'rle':rles})\n",
        "    submission_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "5mlnd1mDsGl6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}