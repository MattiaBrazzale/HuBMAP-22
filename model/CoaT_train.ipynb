{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## HuBMAP-22 Challenge\n",
        "\n",
        "This is my notebook with the training and submitting for the HuBMAP-22 Challenge.\n"
      ],
      "metadata": {
        "id": "6v22Y8t6pqbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements\n",
        "Installing the required packages:"
      ],
      "metadata": {
        "id": "zbKUqAG7pyRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -qq git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "!pip install timm==0.4.12\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "b0KwsgMapx7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading libraries and utilities:"
      ],
      "metadata": {
        "id": "ZSWc9B269Ow-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "from sklearn.model_selection import KFold\n",
        "import segmentation_models_pytorch as smp\n",
        "import tifffile\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from coat import *\n",
        "from daformer import *\n",
        "from utils import seed_everything, make_fold, get_mask\n",
        "import config\n",
        "seed_everything(config.SEED)"
      ],
      "metadata": {
        "id": "nHcMI6CTxO5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataframes and splitting the train set into training and validation set:"
      ],
      "metadata": {
        "id": "_SwQg3LPAFfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(config.TRAIN_PATH+'train.csv')\n",
        "test_df = pd.read_csv(config.TEST_PATH+'test.csv')\n",
        "train_df, val_df = make_fold(num_fold=config.NUM_FOLD, val_fold=config.VAL_FOLD, df=train)"
      ],
      "metadata": {
        "id": "MQYvh1bpAFJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the PyTorch dataset for the images:"
      ],
      "metadata": {
        "id": "4SYY6HyTm1qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuBMAPData(Dataset):\n",
        "    def __init__(self, transform=None, df=train_df):\n",
        "        self.transform = transform\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        temp = self.df.iloc[index]\n",
        "        id = temp['id']\n",
        "        organ = temp['organ']\n",
        "\n",
        "        image = tifffile.imread(config.TRAIN_PATH+str(id)+'.tiff')\n",
        "        mask = get_mask(id, self.df)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "            mask = mask.float()\n",
        "\n",
        "        return image, mask, organ"
      ],
      "metadata": {
        "id": "OW5T0sLOBWDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definining the augmentations for the training set, that we need since we have only a small amount of images:"
      ],
      "metadata": {
        "id": "ET13KBvgm-qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = A.Compose(\n",
        "      [\n",
        "        A.Resize(height=IMG_SIZE, width=IMG_SIZE),\n",
        "        A.Rotate(limit=35, p=0.8),\n",
        "        A.OneOf([\n",
        "            A.HorizontalFlip(p=0.6),\n",
        "            A.VerticalFlip(p=0.6),\n",
        "            A.RandomRotate90(p=0.6)\n",
        "        ], p=1.0),\n",
        "        A.OneOf([\n",
        "            A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
        "            A.GridDistortion(p=0.5),\n",
        "            A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=0.5) \n",
        "        ], p=0.8),\n",
        "        A.ChannelShuffle(p=0.4),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.4),\n",
        "        A.OneOf([\n",
        "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=0.75),\n",
        "            A.RandomBrightnessContrast(p=0.7),\n",
        "            A.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.25, p=0.75)\n",
        "        ], p=0.9),\n",
        "        A.RandomGamma(p=0.6),\n",
        "        ToTensorV2(transpose_mask=True),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMG_SIZE, width=IMG_SIZE),\n",
        "        ToTensorV2(transpose_mask=True),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "elZ1Bdd_Bizv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the datasets and the dataloaders:"
      ],
      "metadata": {
        "id": "lwzgrCVwnO9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = HuBMAPData(transform=train_transform, df=train_df)\n",
        "val_dataset = HuBMAPData(transform=val_transform, df=val_df)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "sru3YutIBnNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoaT \n",
        "\n",
        "Loading the pretrained weights:\n",
        "\n"
      ],
      "metadata": {
        "id": "zTQHrR0T_F5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./coat-pretrained\n",
        "!wget http://vcl.ucsd.edu/coat/pretrained/coat_lite_medium_a750cd63.pth -P ./coat-pretrained\n",
        "!sha256sum ./coat-pretrained/coat_lite_medium_a750cd63.pth"
      ],
      "metadata": {
        "id": "gomVx6RfnyBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model architecture and the function to initialize the model:"
      ],
      "metadata": {
        "id": "xhfOHu7Znun7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RGB(nn.Module):\n",
        "    IMAGE_RGB_MEAN = [0.485, 0.456, 0.406]  \n",
        "    IMAGE_RGB_STD = [0.229, 0.224, 0.225] \n",
        "    \n",
        "    def __init__(self, ):\n",
        "        super(RGB, self).__init__()\n",
        "        self.register_buffer('mean', torch.zeros(1, 3, 1, 1))\n",
        "        self.register_buffer('std', torch.ones(1, 3, 1, 1))\n",
        "        self.mean.data = torch.FloatTensor(self.IMAGE_RGB_MEAN).view(self.mean.shape)\n",
        "        self.std.data = torch.FloatTensor(self.IMAGE_RGB_STD).view(self.std.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (x - self.mean) / self.std\n",
        "        return x\n",
        "\n",
        "    \n",
        "    \n",
        "class Net(nn.Module):\n",
        "\t\n",
        "\t\tdef __init__(self,\n",
        "\t\t\t\t\t\t\t\tencoder=coat_lite_medium,\n",
        "\t\t\t\t\t\t\t\tdecoder=daformer_conv3x3,\n",
        "\t\t\t\t\t\t\t\tencoder_cfg={},\n",
        "\t\t\t\t\t\t\t\tdecoder_cfg={},\n",
        "\t\t\t\t\t\t\t\t):\n",
        "\t\t\t\tsuper(Net, self).__init__()\n",
        "\t\t\t\tdecoder_dim = decoder_cfg.get('decoder_dim', 320)\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.rgb = RGB()\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.encoder = encoder\n",
        "\t\t\t\tencoder_dim = self.encoder.embed_dims\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.decoder = decoder(\n",
        "\t\t\t\t\t\tencoder_dim=encoder_dim,\n",
        "\t\t\t\t\t\tdecoder_dim=decoder_dim,\n",
        "\t\t\t\t)\n",
        "\t\t\t\tself.logit = nn.Sequential(\n",
        "\t\t\t\t\t\tnn.Conv2d(decoder_dim, 1, kernel_size=1),\n",
        "\t\t\t\t\t\tnn.Upsample(scale_factor = 4, mode='bilinear', align_corners=False),\n",
        "\t\t\t\t)\n",
        "\t\t\n",
        "\t\tdef forward(self, batch):\n",
        "\t\t\t\n",
        "\t\t\t\tx = self.rgb(batch)\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\tB, C, H, W = x.shape\n",
        "\t\t\t\tencoder = self.encoder(x)\n",
        "\t\t\t\t\n",
        "\t\t\t\tlast, decoder = self.decoder(encoder)\n",
        "\t\t\t\tlogit = self.logit(last)\n",
        "\t\t\t\t\n",
        "\t\t\t\toutput = {}\n",
        "\t\t\t\tprobability_from_logit = torch.sigmoid(logit)\n",
        "\t\t\t\toutput['probability'] = probability_from_logit\n",
        "\t\t\t\t\n",
        "\t\t\t\treturn output\n",
        "\n",
        "\n",
        "def init_model():\n",
        "\t\t\"\"\"\n",
        "\t\tFunction used to initialize a CoaT model\n",
        "\t\t\"\"\"\n",
        "    encoder = coat_lite_medium()\n",
        "    checkpoint = './coat-pretrained/coat_lite_medium_a750cd63.pth' #pretrained weight available at the CoaT repository\n",
        "    checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
        "    state_dict = checkpoint['model']\n",
        "    encoder.load_state_dict(state_dict,strict=False)\n",
        "    net = Net(encoder=encoder).cuda()\n",
        "    return net"
      ],
      "metadata": {
        "id": "9MGHTZHSsJW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definining the class for the custom loss and the evaluation metric:"
      ],
      "metadata": {
        "id": "qLdjD9HIAkFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss,self).__init__()\n",
        "        self.diceloss = smp.losses.DiceLoss(mode='binary')\n",
        "        self.binloss = smp.losses.SoftBCEWithLogitsLoss(reduction = 'mean' , smooth_factor = 0.1)\n",
        "        self.jaccardloss = smp.losses.JaccardLoss(mode='binary')\n",
        "\n",
        "    def forward(self, output, mask):\n",
        "        dice = self.diceloss(outputs, mask)\n",
        "        bce = self.binloss(outputs, mask)\n",
        "        jaccard = self.jaccardloss(outputs, mask)\n",
        "        loss = dice * 0.3 + jaccard * 0.7\n",
        "        return loss\n",
        "\n",
        "class DiceCoef(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, smooth=1.):\n",
        "        y_true = y_true.view(-1)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        \n",
        "        y_pred = torch.round((y_pred - y_pred.min()) / (y_pred.max() - y_pred.min()))\n",
        "        \n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        dice = (2.0*intersection + smooth)/(y_true.sum() + y_pred.sum() + smooth)\n",
        "        \n",
        "        return dice"
      ],
      "metadata": {
        "id": "dpzsTBBxsJRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "FpTAgipFBevm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_scores=[]\n",
        "val_scores = []\n",
        "best_loss = 999\n",
        "best_score = 0\n",
        "\n",
        "model = init_model().to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.decoder.parameters(), 'lr': 5e-5}, \n",
        "    {'params': model.encoder.parameters(), 'lr': 5e-5},  \n",
        "])\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, \n",
        "                                          max_lr=1e-3,\n",
        "                                          epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
        "\n",
        "loss_func = CustomLoss()\n",
        "dice_coe = DiceCoef()\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "\n",
        "    # ------- Train ------- #\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    score = 0\n",
        "\n",
        "    for batch_idx, (img, mask, organ) in enumerate(train_loader):\n",
        "        \n",
        "        img = img.float().to(device=DEVICE)\n",
        "        mask = mask.float().to(device=DEVICE)\n",
        "        outputs = model(img)['probability']   \n",
        "\n",
        "        loss = loss_func(outputs, mask)\n",
        "        loss.backward()\n",
        "        \n",
        "        if ((batch_idx+1)*BATCH_SIZE % ACCUMULATION == 0) | ((batch_idx+1) == len(train_df)):\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        train_loss += loss.detach()\n",
        "        score += dice_coe(outputs,mask).item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    score /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    train_scores.append(score)\n",
        "    print(f\"FOLD: {val_fold}, EPOCH: {epoch+1}, Train_Loss: {train_loss} , Dice Value: {score}\") #\n",
        "\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # ------ Validation ------ #\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        valid_loss = 0\n",
        "        val_score = 0\n",
        "\n",
        "        for i, (img, mask, organ) in enumerate(val_loader):\n",
        "\n",
        "            img = img.float().to(device=DEVICE)\n",
        "            mask = mask.float().to(device=DEVICE)\n",
        "            outputs = model(img)['probability']\n",
        "\n",
        "            loss = loss_func(outputs, mask)\n",
        "            valid_loss += loss.item()\n",
        "            val_score += dice_coe(outputs,mask).item()\n",
        "\n",
        "        valid_loss /= len(val_loader)\n",
        "        val_losses.append(valid_loss)\n",
        "\n",
        "        val_score /= len(val_loader)\n",
        "        val_scores.append(val_score)\n",
        "\n",
        "        print(f\"FOLD: {val_fold}, EPOCH: {epoch+1}, Valid_Loss: {valid_loss} , Valid Dice Value: {val_score}\") \n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if val_score > best_score:\n",
        "        best_score = val_score\n",
        "        torch.save(model.state_dict(), f\"./FOLD{fold}_best_score_epoch{epoch+1}.pth\")\n",
        "        print(f\"Saved model for best score : FOLD{fold}_best_score_epoch{epoch+1}.pth\")\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        torch.save(model.state_dict(), f\"./FOLD{fold}_best_loss_epoch{epoch+1}.pth\")\n",
        "        print(f\"Saved model for best loss : FOLD{fold}_best_loss_epoch{epoch+1}.pth\")    \n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        torch.save(model.state_dict(), f\"/kaggle/working/FOLD{fold}_epoch_{epoch+1}.pth\")\n",
        "        print(f\"Saved model for current epoch: FOLD{fold}_epoch_{epoch+1}.pth\")\n"
      ],
      "metadata": {
        "id": "TugDBkdaJVZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}